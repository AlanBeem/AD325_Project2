{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AD325 Project 2\n",
    "#### Recommendation System\n",
    "11/06/2024\n",
    "Alan M H Beem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial results were inconsistent, much of which was to do with priority-ties in heap, assembled in a different order depending on hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568097721116867455\n"
     ]
    }
   ],
   "source": [
    "print(hash(\"string_test\"))  # this varies with each restart of the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T18:16:53.808648Z",
     "start_time": "2024-11-01T18:16:53.521666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recommendations using 'sum' update technique, and biasing as a function of game popularity, and game similarity:\n",
      "Using collision avoidance technique: sepa\n",
      "Recommendations for user1: ['Battlefield', 'The Last of Us', 'Splatoon']\n",
      "Recommendations for user2: ['Bayonetta', 'Splatoon', 'Gears of War']\n",
      "Recommendations for user3: ['League of Legends', 'Resident Evil', 'Bayonetta']\n",
      "Recommendations for user4: ['Silent Hill', 'Half-Life', 'Splatoon']\n",
      "Recommendations for user5: ['Bayonetta', 'Resident Evil', 'Half-Life']\n",
      "Recommendations for user6: ['Red Dead Redemption 2', 'Dragon Age', 'Metal Gear Solid']\n",
      "Recommendations for user7: ['Bioshock', 'Super Mario Bros', 'Animal Crossing']\n",
      "Insertion Time: 0.0007300376892089844\n",
      "Retrieval Time: 0.0007300376892089844\n",
      "UserHashTable collisions since last rehash: 0\n",
      "Total collisions since last rehashes: 180 (this includes all HashTables used in comparisons of users' games lists)\n",
      "Probe time: 0.0002529621124267578\n"
     ]
    }
   ],
   "source": [
    "from main_script import main, demonstration\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with open addressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimental_hash_table import ExperimentalHashTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "HashTable initial size: 97, data size: N=1000\n",
      "\n",
      "\n",
      "HashTable size after operation: 3203, data size: N=1000\n",
      "Average number of collisions for N={N} shuffled order, this inserts and retrieves a key, then inserts half the keys, then inserts the last key of the first half, and the rest of the keys (expect a collision for each), 100 iterations\n",
      "separate chaining: 1.0 \n",
      "linear: 560.22 \n",
      "quadratic: 34.37 \n",
      "double: 2.72 \n",
      "prime: 116.75 \n",
      "3/2: 89.32 \n",
      "to Euler's number: 16.05 \n",
      "exponential: 11.15 \n",
      "^e 2: 16.83 the second hash is computed by the first hash ^ Euler's number + (1 if even, else 0)\n",
      "cubic: 14.05 \n",
      "quartic: 8.43 \n",
      "quintic: 6.11 \n",
      "sextic: 5.1 \n",
      "septic: 4.72 \n",
      "octic: 6.92 \n",
      "nonic: 4.46 \n",
      "decic: 3.25 \n",
      "11: 3.5 \n",
      "12: 3.06 \n",
      "13: 2.89 \n",
      "14: 5.02 \n",
      "15: 3.19 \n",
      "16: 2.98 \n",
      "17: 3.69 \n",
      "18: 3.45 \n",
      "19: 2.69 \n",
      "20: 2.93 \n",
      "rand: 1.58 \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "hash_table = ExperimentalHashTable(97, 'sep')\n",
    "print(f\"\\n\\nHashTable initial size: {len(hash_table.hash_table)}, data size: N={N}\")\n",
    "iterations = 100\n",
    "probing_functions_list = ['separate chaining', 'linear', 'quadratic', 'double', 'prime', '3/2', \"to Euler's number\", 'exponential',  \"^e 2\", 'cubic', 'quartic', 'quintic', 'sextic', 'septic', 'octic', 'nonic', 'decic', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', 'rand']\n",
    "collisions_data = [0 for each in probing_functions_list]\n",
    "probe_distances = [set() for each in probing_functions_list]\n",
    "\n",
    "index_keys = [n for n in range(N + 1)]\n",
    "total_inserts = 0\n",
    "\n",
    "for _ in range(iterations):\n",
    "    index_keys_copy = index_keys.copy()\n",
    "    random.shuffle(index_keys_copy)\n",
    "    for probing_function, each_set in zip(probing_functions_list, probe_distances):\n",
    "        hash_table = ExperimentalHashTable(97, probing_function)\n",
    "        hash_table.insert(index_keys_copy[0], '')\n",
    "        hash_table.retrieve(index_keys_copy[0])\n",
    "        for key in index_keys_copy[0:len(index_keys_copy) // 2 + 1]:\n",
    "            hash_table.insert(key, '')\n",
    "        for key in index_keys_copy[len(index_keys_copy) // 2:]:\n",
    "            hash_table.insert(key, '')\n",
    "        collisions_data[probing_functions_list.index(probing_function)] += hash_table.collision_count\n",
    "        for each in hash_table.probe_distances:\n",
    "            each_set.add(tuple(each))\n",
    "print(f\"\\n\\nHashTable size after operation: {len(hash_table.hash_table)}, data size: N={N}\")\n",
    "print(\"Average number of collisions for N={N} shuffled order, this inserts and retrieves a key, then inserts half the keys, then inserts the last key of the first half, and the rest of the keys (expect a collision for each), 100 iterations\") \n",
    "for each_func, each_datum in zip(probing_functions_list, collisions_data):\n",
    "    print(f\"{each_func}: {each_datum / iterations} {'the second hash is computed by the first hash ^ Euler\\'s number + (1 if even, else 0)' if each_func == '^e 2' else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural ordering of games for consistent results\n",
    "\n",
    "The games are naturally ordered by various means, such as alphabetical, here, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between collision avoidance techniques, collisions\n",
    "\n",
    "Separate chaining can have at most a number of collisions per insert equal to the longest chain, and keeping the table to a load factor of up to 1 keeps these relatively short. Separate chaining has the advantage of collisions not 'spilling over' into the rest of the table. The best open addressing collision avoidance technique is double, as it produces a uniform second hash function (?). Generally, over the other techniques, it seems that the larger power of the exponent applied to the hash the better the function is at avoiding collisions, to a point, and likely this threshold depends on the size of the hash table relative to the expected values of the probe function (in the experiment above, this seems to occur between quartic and quintic probing). Second to double hashing is randomly hashing steps; some sequences of random steps are worse than others, in terms of collisions, but they're mostly pretty good. Prime probing (each step is the kth prime, starting from 1) is between 3/2 and to Euler's number (~2.718).\n",
    "### MaxHeap\n",
    "\n",
    "The max heap can't be guaranteed to provide unique games unless a scheme is implemented to update values (which is handled in a linear time subroutine in UserMaxHeap). The entries are updated by keeping the maximum priority, or forming a sum of priorities. I think the sum technique makes more sense: \n",
    "##### MaxHeap Maximum or Sum: Sum\n",
    "Looking at the recommendations by the 'keep maximum' update technique, if the number of games of a user_j that are disjoint from the games of a user_i is equal to or greater than the value of top_n, and that user (user_j) is the most similar to user_i, then all the recommendations will be from the information of only one user (user_j). Additionally, we cannot distinguish the priorities of the games resulting from such a selection from the heap.\n",
    "\n",
    "The recommendations by the sum update technique incorporate information from all users.\n",
    "\n",
    "A hypothesis to support only treating some pairs of users as similar is either contradictory to the computation as performed, or forms an equivalence relation that could cluster computations. If this forms an equivalence relation, then the data ought to be processed such that clusters of users are formed (eq. rel.; a hash) before executing computations with complexity proportional to Cartesian products. We might still want to find the summed Jaccard similarity between all users within each cluster.\n",
    "\n",
    "Without such a hypothesis, I think that it makes more sense to make recommendations as a function of the sum of Jaccard similarity for disjoint games over all users in the dataset.\n",
    "\n",
    "Such hypotheses may be of ample supply, and here is where one could fit in additional logic or descriptions of user similarity. Though there are likely ample practical hypotheses from fields such as related to marketing, an implementation such as in this project could be used with such hypothetical clusters, and where some data such as outcomes could determine whether a purchased game was liked (or, ought to have been recommended), arbitrary hypotheses (eq. rel., hash functions) could be distinguished on the basis of correct predictions. Regarding real-world data, such hypothetical comparisons would be biased due to sampling only the actually purchased games (another point of entry for ML/description of user behavior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "HashTable initial size: 97, data size: N=1000\n",
      "\n",
      "\n",
      "HashTable size after operation: 3203, data size: N=1000\n",
      "Average number of collisions for N=1000 shuffled order, this inserts then retrieves the last key, inserts a duplicate of the second to last key, and retrieves and inserts it again, 100 iteration\n",
      "separate chaining: 2.0 \n",
      "linear: 1000.98 \n",
      "quadratic: 43.86 \n",
      "double: 12.52 \n",
      "prime: 182.18 \n",
      "3/2: 121.36 \n",
      "to Euler's number: 20.48 \n",
      "exponential: 14.02 \n",
      "^e 2: 20.46 the second hash is computed by the first hash ^ Euler's number + (1 if even, else 0)\n",
      "cubic: 16.8 \n",
      "quartic: 11.02 \n",
      "quintic: 9.04 \n",
      "sextic: 7.68 \n",
      "septic: 7.3 \n",
      "octic: 8.44 \n",
      "nonic: 7.09 \n",
      "decic: 5.0 \n",
      "11: 5.01 \n",
      "12: 6.73 \n",
      "13: 5.54 \n",
      "14: 6.63 \n",
      "15: 6.82 \n",
      "16: 5.0 \n",
      "17: 6.48 \n",
      "18: 5.96 \n",
      "19: 5.0 \n",
      "20: 5.0 \n",
      "rand: 4.5 \n",
      "\n",
      "\n",
      "HashTable initial size: 97, data size: N=1000\n",
      "\n",
      "\n",
      "HashTable size after operation: 3203, data size: N=500\n",
      "Average number of collisions for N={N} shuffled order, this inserts and retrieves a key, then inserts half the keys, retrieving and inserting the last inserted key (midpoint of list), then adds the rest of the keys (expect a collision for each), 100 iterations\n",
      "separate chaining: 1.0 \n",
      "linear: 462.75 \n",
      "quadratic: 31.86 \n",
      "double: 2.08 \n",
      "prime: 97.86 \n",
      "3/2: 76.73 \n",
      "to Euler's number: 15.02 \n",
      "exponential: 10.34 \n",
      "^e 2: 15.26 the second hash is computed by the first hash ^ Euler's number + (1 if even, else 0)\n",
      "cubic: 12.53 \n",
      "quartic: 7.45 \n",
      "quintic: 5.9 \n",
      "sextic: 4.32 \n",
      "septic: 3.73 \n",
      "octic: 5.17 \n",
      "nonic: 3.46 \n",
      "decic: 3.31 \n",
      "11: 2.4 \n",
      "12: 2.8 \n",
      "13: 2.37 \n",
      "14: 3.39 \n",
      "15: 2.77 \n",
      "16: 2.42 \n",
      "17: 3.47 \n",
      "18: 3.13 \n",
      "19: 2.38 \n",
      "20: 2.42 \n",
      "rand: 1.63 \n"
     ]
    }
   ],
   "source": [
    "hash_table = ExperimentalHashTable(97, 'sep')\n",
    "print(f\"\\n\\nHashTable initial size: {len(hash_table.hash_table)}, data size: N=1000\")\n",
    "iterations = 100\n",
    "probing_functions_list = ['separate chaining', 'linear', 'quadratic', 'double', 'prime', '3/2', \"to Euler's number\", 'exponential',  \"^e 2\", 'cubic', 'quartic', 'quintic', 'sextic', 'septic', 'octic', 'nonic', 'decic', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', 'rand']\n",
    "collisions_data = [0 for each in probing_functions_list]\n",
    "\n",
    "N = 1000\n",
    "index_keys = [n for n in range(N + 1)]\n",
    "for _ in range(iterations):\n",
    "    index_keys_copy = index_keys.copy()\n",
    "    random.shuffle(index_keys_copy)\n",
    "    for probing_function in probing_functions_list:\n",
    "        hash_table = ExperimentalHashTable(97, probing_function)\n",
    "        # hash_table.rehash_increment = lambda p: 100\n",
    "        hash_table.insert(index_keys_copy[-1], '')\n",
    "        hash_table.retrieve(index_keys_copy[-1])\n",
    "        for key in index_keys_copy:\n",
    "            hash_table.insert(key, '')\n",
    "        hash_table.insert(index_keys_copy[-2], '')\n",
    "        retrieval = hash_table.retrieve(index_keys_copy[-2])\n",
    "        hash_table.insert(retrieval[0], retrieval[1])\n",
    "        collisions_data[probing_functions_list.index(probing_function)] += hash_table.collision_count\n",
    "print(f\"\\n\\nHashTable size after operation: {len(hash_table.hash_table)}, data size: N={N}\")\n",
    "print(\"Average number of collisions for N=1000 shuffled order, this inserts then retrieves the last key, inserts a duplicate of the second to last key, and retrieves and inserts it again, 100 iteration\")\n",
    "for each_func, each_datum in zip(probing_functions_list, collisions_data):\n",
    "    print(f\"{each_func}: {each_datum / iterations} {'the second hash is computed by the first hash ^ Euler\\'s number + (1 if even, else 0)' if each_func == '^e 2' else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum probe distance for separate chaining: 1\n",
      "maximum probe distance for linear: 7\n",
      "maximum probe distance for quadratic: 5\n",
      "maximum probe distance for double: 7\n",
      "maximum probe distance for prime: 9\n",
      "maximum probe distance for 3/2: 11\n",
      "maximum probe distance for to Euler's number: 10\n",
      "maximum probe distance for exponential: 6\n",
      "maximum probe distance for ^e 2: 6\n",
      "maximum probe distance for cubic: 5\n",
      "maximum probe distance for quartic: 7\n",
      "maximum probe distance for quintic: 6\n",
      "maximum probe distance for sextic: 4\n",
      "maximum probe distance for septic: 5\n",
      "maximum probe distance for octic: 5\n",
      "maximum probe distance for nonic: 6\n",
      "maximum probe distance for decic: 4\n",
      "maximum probe distance for 11: 7\n",
      "maximum probe distance for 12: 4\n",
      "maximum probe distance for 13: 5\n",
      "maximum probe distance for 14: 6\n",
      "maximum probe distance for 15: 3\n",
      "maximum probe distance for 16: 5\n",
      "maximum probe distance for 17: 5\n",
      "maximum probe distance for 18: 6\n",
      "maximum probe distance for 19: 8\n",
      "maximum probe distance for 20: 6\n",
      "maximum probe distance for rand: 6\n"
     ]
    }
   ],
   "source": [
    "# regarding Experiment 3.1.2\n",
    "for probe_function, each_set in zip(probing_functions_list, probe_distances):\n",
    "    print(f\"maximum probe distance for {probe_function}: {max([element[1] for element in each_set])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# control 2\n",
    "control_ht = ExperimentalHashTable(2, 'linear')\n",
    "control_ht.insert(1, '')\n",
    "control_ht.insert(1, '')\n",
    "print(control_ht.collision_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code using builtins: (Not for grading, but it is concise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by sum: user6: [(Red Dead Redemption 2, 0.40476190476194474), (Dragon Age, 0.38624338624344623), (World of Warcraft, 0.38624338624342625), (Metal Gear Solid, 0.38624338624342625), (The Legend of Zelda, 0.3648148148149648)]\n",
      "by sum: user4: [(Silent Hill, 0.4488636363637264), (Half-Life, 0.4438636363637264), (Splatoon, 0.43250000000009003), (Dota 2, 0.31250000000004), (The Last of Us, 0.30636363636375635)]\n",
      "by sum: user1: [(The Last of Us, 0.40952380952392947), (Battlefield, 0.4095238095238995), (Splatoon, 0.3380952380953281), (Gears of War, 0.3380952380952781), (Bloodborne, 0.3380952380952781)]\n",
      "by sum: user5: [(Bayonetta, 0.5369047619049618), (Resident Evil, 0.4786231884059171), (Half-Life, 0.39066022544292406), (Portal, 0.3619047619048219), (Fire Emblem, 0.36190476190480186)]\n",
      "by sum: user2: [(Bayonetta, 0.50489365706777), (Splatoon, 0.4673913043479161), (Gears of War, 0.41666666666670665), (Bloodborne, 0.41666666666670665), (Cyberpunk 2077, 0.3863636363636763)]\n",
      "by sum: user7: [(Bioshock, 0.5846014492754523), (Super Mario Bros, 0.4571428571429471), (Animal Crossing, 0.40952380952386946), (The Legend of Zelda, 0.3920088566829197), (League of Legends, 0.3209109730849761)]\n",
      "by sum: user3: [(League of Legends, 0.49814814814823816), (Resident Evil, 0.4440740740741941), (Bayonetta, 0.4422222222224222), (Bioshock, 0.407037037037127), (Sekiro, 0.3981481481481881)]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "game_cumulative_list = []\n",
    "user_data_sets = {}  # dict()\n",
    "with open('user_item_data.csv', 'r') as open_file:\n",
    "    csv_reader = csv.reader(open_file)\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        user = row[0]\n",
    "        game = row[1]\n",
    "        game_cumulative_list.append(game)\n",
    "        if user not in user_data_sets:\n",
    "            user_data_sets[user] = set()\n",
    "        user_data_sets[user].add(game)\n",
    "open_file.close()\n",
    "# print(user_data_sets)\n",
    "\n",
    "\n",
    "class GamePriority:\n",
    "    def __init__(self, game: str, priority: float):\n",
    "        self.game = game\n",
    "        self.priority = priority\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"({self.game}, {self.priority})\"\n",
    "\n",
    "\n",
    "class GamePrioritySum(GamePriority):\n",
    "    def equals(self, other) -> bool:\n",
    "        if isinstance(other, self.__class__):\n",
    "            if self.game == other.game:\n",
    "                self.priority += other.priority\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class GamePriorityMaximum(GamePriority):  # See K-maps - Make K-map efficiency analyzer that compares all possible alignable (could be equivalent lines) parts, but also numerically, if applicable)\n",
    "    def equals(self, other) -> bool:\n",
    "        if isinstance(other, self.__class__):\n",
    "            if self.game == other.game:\n",
    "                self.priority = max(self.priority, other.priority)\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class GameList(list):\n",
    "    def append(self, object):\n",
    "        for each in self:\n",
    "            if each.equals(object):\n",
    "                break\n",
    "        else:\n",
    "            return super().append(object)\n",
    "        return None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{[str(each) for each in self]}\"\n",
    "\n",
    "jaccard_matrix = [[0.3 for i in range(len(user_data_sets))] for j in range(len(user_data_sets))]\n",
    "\n",
    "user_recs_list_sum = []\n",
    "user_recs_list_max = []\n",
    "\n",
    "for i, user_i in zip(range(len(jaccard_matrix)), user_data_sets.keys()):\n",
    "    user_recs_list_sum.append(GameList())\n",
    "    user_recs_list_max.append(GameList())\n",
    "    for j, user_j in zip(range(len(jaccard_matrix[i])), user_data_sets.keys()):\n",
    "        if i != j:\n",
    "            jaccard_matrix[i][j] = len(user_data_sets[user_i].intersection(user_data_sets[user_j])) / len(user_data_sets[user_i].union(user_data_sets[user_j]))\n",
    "            for game in (user_data_sets[user_j] - user_data_sets[user_i]):\n",
    "                game_bias = (game_cumulative_list.count(game) / len(game_cumulative_list)) * 0.000000000001\n",
    "                user_recs_list_sum[-1].append(GamePrioritySum(game, jaccard_matrix[i][j] + game_bias))\n",
    "                user_recs_list_max[-1].append(GamePriorityMaximum(game, jaccard_matrix[i][j] + game_bias))\n",
    "\n",
    "for rec_i in range(len(user_recs_list_sum)):\n",
    "    user_recs_list_sum[rec_i].sort(key=lambda p: -p.priority)\n",
    "for rec_j in range(len(user_recs_list_max)):\n",
    "    user_recs_list_max[rec_j].sort(key=lambda p: -p.priority)\n",
    "\n",
    "for each_rec_list_sum, each_rec_list_max, user in zip(user_recs_list_sum, user_recs_list_max, user_data_sets.keys()):\n",
    "    print(f\"by sum: {user}: {each_rec_list_sum[0:5]}\")\n",
    "    # print(f\"by max: {user}: {each_rec_list_max[0:5]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
